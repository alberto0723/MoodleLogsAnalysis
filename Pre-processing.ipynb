{"cells":[{"cell_type":"markdown","metadata":{"id":"-MEpCOxBHW8D"},"source":["INTEGRANTES:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qejlOlcAN8E"},"outputs":[],"source":["!pip install pyspark"]},{"cell_type":"markdown","metadata":{"id":"kJweQy7fA7PY"},"source":["# Set up"]},{"cell_type":"markdown","metadata":{"id":"Ws20ucPlBnnx"},"source":["We will import the files needed for the exercises as follows:"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Gain access to the source files in GDrive\n","drive.mount('/content/drive')"],"metadata":{"id":"2ujQOJmw2EaO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Set Data Path\n","dataPath = \"/content/drive/MyDrive/Data/2425Q1-DBDP1\"\n","# Show Data Folders\n","os.listdir(dataPath)"],"metadata":{"id":"ElDZvAma2HpA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark import SparkConf\n","from pyspark.sql import SparkSession\n","\n","if not 'spark' in globals():\n","  # Create the configuration\n","  #   Replace the \"*\" with the number of parallel processors your job requires (Google Collab offers maximum 2)\n","  conf = SparkConf() \\\n","      .set(\"spark.master\", \"local[*]\") \\\n","      .set(\"spark.app.name\", \"Analysis of video visualizations\")\n","  spark = SparkSession.builder.config(conf=conf).getOrCreate()\n","else:\n","  print(\"Spark session already exists!!!\")"],"metadata":{"id":"f-UTYrAQ6HL1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Constants Definition"],"metadata":{"id":"Nqnl-BBGJJ_D"}},{"cell_type":"code","source":["courseName=\"Disseny de Bases de Dades\"\n","firstClassDate=\"2024-09-13 14:00:00\"\n","examDate=\"2024-11-13 12:00:00\"\n","lastSessionDate=\"2024-10-23\"\n","plannedExamDate=\"2024-11-06\"\n","sessionDay=\"Wed\"\n","beforeExamMove=7\n","irrelevantUsers=[]\n","irrelevantSlides=[]"],"metadata":{"id":"Cvplgnu7Ig1G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preparation"],"metadata":{"id":"FlFNZoruQ0ay"}},{"cell_type":"markdown","source":["## Clean Logs"],"metadata":{"id":"sAuM_wAxcbTD"}},{"cell_type":"code","source":["from pyspark.sql.functions import try_to_timestamp, lit\n","\n","# Obtain classes list\n","classes = spark.read.csv(dataPath+\"/InClass.csv\", header='true', inferSchema='true', sep=';') \\\n","                        .withColumn(\"Start\", try_to_timestamp(\"Start\", lit('d/MM/yy, HH:mm'))) \\\n","                        .withColumn(\"End\", try_to_timestamp(\"End\", lit('d/MM/yy, HH:mm'))) \\\n","                        .cache()\n","print(\"Classes loaded: \"+str(classes.count()))\n","classes.show(n=5, truncate=False)"],"metadata":{"id":"ezTGoaqxTKVn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import try_to_timestamp, trim, lit, dayofweek, hour\n","\n","# Obtain Moodle logs dataset\n","# Trim user name and event names (not really necessary)\n","# Transform the timestamp into the right data type (to deal with days of one digit, only one \"d\" must be provided in the format)\n","# Take the right time interval of the log\n","load = spark.read.csv(dataPath+\"/logs.csv\", header='true', inferSchema='true', sep=',') \\\n","      .drop(\"Affected user\", \"IP address\", \"Origin\", \"Description\", \"Event name\") \\\n","      .toDF(*[\"Timestamp\", \"Username\", \"Event\", \"Component\"]) \\\n","      .withColumn(\"Username\", trim(\"Username\")) \\\n","      .withColumn(\"Event\", trim(\"Event\")) \\\n","      .withColumn(\"Timestamp\", try_to_timestamp(\"Timestamp\", lit('d/MM/yy, HH:mm'))) \\\n","      .where(\"Timestamp>'\"+firstClassDate+\"'\") \\\n","      .where(\"Timestamp<'\"+examDate+\"'\") \\\n","      .where(\"Username NOT IN ('\"+(\"','\".join(irrelevantUsers))+\"')\") \\\n","      .withColumn(\"Weekday\", dayofweek(\"Timestamp\")) \\\n","      .withColumn(\"Hour\", hour(\"Timestamp\")) \\\n","      .cache()\n","print(\"Logs loaded: \", load.count())\n","load.show(5)"],"metadata":{"id":"R8lqZpz5Q60U","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove log entries during classes\n","logs = load.join(classes, [classes.Start<=load.Timestamp, load.Timestamp<=classes.End], \"leftanti\").cache()\n","print(\"Logs excluding classes: \", logs.count())\n","logs.show(5)"],"metadata":{"id":"1ZKhfWs0Vf-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Logins"],"metadata":{"id":"rUQxrzBeXbTZ"}},{"cell_type":"code","source":["from pyspark.sql.functions import substr, lit\n","\n","#Remove irrelevant rows and columns\n","reducedLoginLogs = logs \\\n","                  .where(\"Component='System'\") \\\n","                  .drop(\"Component\") \\\n","                  .withColumn(\"Event\", substr(\"Event\", lit(9))) \\\n","                  .where(\"Event='\"+courseName+\"'\") \\\n","                  .drop(\"Event\") \\\n","                  .cache()\n","print(\"Reduced login logs: \", reducedLoginLogs.count())\n","reducedLoginLogs.show(n=5, truncate=False)"],"metadata":{"id":"5DA5YIfiXZUW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import Row\n","\n","# 1=Sunday and the class was on Wedday\n","daysMap={'1': \"Clase+4\", '2': \"Clase+5\", '3': \"Clase+6\", '4': \"Clase\", '5': \"Clase+1\", '6': \"Clase+2\", '7': \"Clase+3\"}\n","\n","loginLogsPerWeekday = reducedLoginLogs.rdd.map(lambda x: Row(Weekday=daysMap[str(x.Weekday)])).toDF() \\\n","                                    .groupBy(\"Weekday\").count() \\\n","                                    .sort(\"Weekday\").cache()\n","loginLogsPerWeekday.toPandas().to_csv(dataPath+'/loginLogsPerWeekday.csv')\n","loginLogsPerWeekday.show()"],"metadata":{"id":"G1nCr2QItE39"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loginLogsPerHour = reducedLoginLogs.groupBy(\"Hour\").count().sort(\"Hour\").cache()\n","loginLogsPerHour.toPandas().to_csv(dataPath+'/loginLogsPerHour.csv')\n","loginLogsPerHour.show()"],"metadata":{"id":"Kg6taKGPtNyv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove first irrelevan characters of the Event name\n","# Take only the current course\n","# Take only one per different day\n","from pyspark.sql.functions import to_date\n","\n","cleanLoginLogs = reducedLoginLogs \\\n","                        .withColumn(\"Timestamp\", to_date(\"Timestamp\")) \\\n","                        .distinct() \\\n","                        .cache()\n","\n","print(\"Clean login logs: \", cleanLoginLogs.count())\n","cleanLoginLogs.show(n=5, truncate=False)"],"metadata":{"id":"r59xjzMTXgYT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summarize video logs cardinalities\n","print(\"Total logs: \"+str(logs.count()))\n","print(\"Total login logs after reduction: \"+str(reducedLoginLogs.count())) # This should be smaller, because removes many other log events\n","print(\"Total login logs after cleaning: \"+str(cleanLoginLogs.count())) # This should be smaller, because groups many dates"],"metadata":{"id":"GYzlJIYxdS21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Release memory\n","reducedLoginLogs.unpersist()\n","globals().pop('reducedLoginLogs')\n","print(\"Variables removed!!!\")"],"metadata":{"id":"uTEXdCr3deS2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Video"],"metadata":{"id":"zERTX1w8gCOX"}},{"cell_type":"code","source":["#Remove irrelevant rows and columns\n","reducedVideoLogs = logs \\\n","                  .where(\"Component='URL'\") \\\n","                  .drop(\"Component\") \\\n","                  .cache()\n","print(\"Reduced video logs: \", reducedVideoLogs.count())\n","reducedVideoLogs.show(n=5, truncate=False)"],"metadata":{"id":"Wou0v_PGSOLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import Row\n","\n","# 1=Sunday and the class was on Wedday\n","daysMap={'1': \"Clase+4\", '2': \"Clase+5\", '3': \"Clase+6\", '4': \"Clase\", '5': \"Clase+1\", '6': \"Clase+2\", '7': \"Clase+3\"}\n","videoLogsPerWeekday = reducedVideoLogs.rdd.map(lambda x: Row(Weekday=daysMap[str(x.Weekday)])).toDF() \\\n","                            .groupBy(\"Weekday\").count() \\\n","                            .sort(\"Weekday\").cache()\n","videoLogsPerWeekday.toPandas().to_csv(dataPath+'/videoLogsPerWeekday.csv')\n","videoLogsPerWeekday.show()"],"metadata":{"id":"H6Ex6R_sugFK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["videoLogsPerHour = reducedVideoLogs.groupBy(\"Hour\").count().sort(\"Hour\").cache()\n","videoLogsPerHour.toPandas().to_csv(dataPath+'/videoLogsPerHour.csv')\n","videoLogsPerHour.show()"],"metadata":{"id":"hU-O1XvJuoXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove first irrelevan characters of the Event name\n","# Take only the first visualization of each video\n","from pyspark.sql.functions import substr, lit, min, count\n","\n","cleanVideoLogs = reducedVideoLogs \\\n","                        .withColumn(\"Event\", substr(\"Event\", lit(13))) \\\n","                        .groupBy(\"Username\",\"Event\").agg(min(\"Timestamp\").alias(\"Timestamp\"), count(\"*\").alias(\"VideoRepetitions\")) \\\n","                        .cache()\n","print(\"Clean video logs: \", cleanVideoLogs.count())\n","cleanVideoLogs.show(n=5, truncate=False)"],"metadata":{"id":"SgTov0xMUMPK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summarize video logs cardinalities\n","print(\"Total logs: \"+str(logs.count()))\n","print(\"Total video logs after reduction: \"+str(reducedVideoLogs.count())) # This should be smaller, because removes many other log events\n","print(\"Total video logs after cleaning: \"+str(cleanVideoLogs.count())) # This should be smaller, because groups many dates"],"metadata":{"id":"lakxmq8HbVwU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Release memory\n","reducedVideoLogs.unpersist()\n","globals().pop('reducedVideoLogs')\n","print(\"Variables removed!!!\")"],"metadata":{"id":"His7oBItbmAe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Slide"],"metadata":{"id":"FSAsDqfqgG12"}},{"cell_type":"code","source":["#Remove irrelevant rows and columns\n","reducedSlideLogs = logs \\\n","                  .where(\"Component='File'\") \\\n","                  .drop(\"Component\") \\\n","                  .cache()\n","print(\"Reduced slide logs: \", reducedSlideLogs.count())\n","reducedSlideLogs.show(n=5, truncate=False)"],"metadata":{"id":"ss4dX7Jlfdxx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import Row\n","\n","# 1=Sunday and the class was on Wedday\n","daysMap={'1': \"Clase+4\", '2': \"Clase+5\", '3': \"Clase+6\", '4': \"Clase\", '5': \"Clase+1\", '6': \"Clase+2\", '7': \"Clase+3\"}\n","\n","slideLogsPerWeekday = reducedSlideLogs.rdd.map(lambda x: Row(Weekday=daysMap[str(x.Weekday)])).toDF() \\\n","                                    .groupBy(\"Weekday\").count() \\\n","                                    .sort(\"Weekday\").cache()\n","slideLogsPerWeekday.toPandas().to_csv(dataPath+'/slideLogsPerWeekday.csv')\n","slideLogsPerWeekday.show()"],"metadata":{"id":"7FjJqjimuuri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["slideLogsPerHour = reducedSlideLogs.groupBy(\"Hour\").count().sort(\"Hour\").cache()\n","slideLogsPerHour.toPandas().to_csv(dataPath+'/slideLogsPerHour.csv')\n","slideLogsPerHour.show()"],"metadata":{"id":"YxmFlZkpu0U8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Remove first irrelevan characters of the Event name\n","# Take only the first visualization of each video\n","from pyspark.sql.functions import startswith, substr, lit, min, count\n","\n","cleanSlideLogs = reducedSlideLogs \\\n","                        .where(startswith(\"Event\", lit(\"File: Slides: \"))) \\\n","                        .withColumn(\"Event\", substr(\"Event\", lit(15))) \\\n","                        .where(\"Event NOT IN ('\"+(\"','\".join(irrelevantSlides))+\"')\") \\\n","                        .groupBy(\"Username\",\"Event\").agg(min(\"Timestamp\").alias(\"Timestamp\"), count(\"*\").alias(\"SlideRepetitions\")) \\\n","                        .cache()\n","\n","print(\"Clean slide logs: \", cleanSlideLogs.count())\n","cleanSlideLogs.show(n=5, truncate=False)"],"metadata":{"id":"UzMj8tJChKDg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summarize logs cardinalities\n","print(\"Total logs: \"+str(logs.count()))\n","print(\"Total slide logs after reduction: \"+str(reducedSlideLogs.count())) # This should be smaller, because removes many other log events\n","print(\"Total slide logs after cleaning: \"+str(cleanSlideLogs.count())) # This should be smaller, because removes irrelevant slides and groups many dates"],"metadata":{"id":"-loJhVGupwrv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Release memory\n","logs.unpersist()\n","reducedSlideLogs.unpersist()\n","# Remove variables to avoid unwillingly accessing them\n","globals().pop('logs')\n","globals().pop('reducedSlideLogs')\n","print(\"Variables removed!!!\")"],"metadata":{"id":"hAriGDkzgXce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate Users"],"metadata":{"id":"Zy_TfIwUsEOP"}},{"cell_type":"code","source":["userList = cleanVideoLogs.select(\"Username\") \\\n","                      .unionAll(cleanSlideLogs.select(\"Username\")) \\\n","                      .distinct() \\\n","                      .sort(\"Username\") \\\n","                      .coalesce(1) \\\n","                      .cache()\n","print(\"Total users: \", userList.count())\n","userList.show(n=5, truncate=False)\n","userList.write.csv(path=dataPath+\"/Usernames\", mode=\"overwrite\", header=True)\n","print(\"CSV written!!!\")\n","userList.unpersist()\n","globals().pop('userList')\n","print(\"Variables removed!!!\")"],"metadata":{"id":"gOg11rHZqzme"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate Elements List"],"metadata":{"id":"Pa_HlmBMgpqJ"}},{"cell_type":"markdown","source":["### Video"],"metadata":{"id":"X0oboXJNleJc"}},{"cell_type":"code","source":["videoList = cleanVideoLogs.select(\"Event\") \\\n","                      .distinct() \\\n","                      .coalesce(1) \\\n","                      .cache()\n","print(\"Total de videos: \", videoList.count())\n","videoList.show(n=5, truncate=False)\n","videoList.write.csv(path=dataPath+\"/Videos\", mode=\"overwrite\", header=True)\n","print(\"CSV written!!!\")\n","videoList.unpersist()\n","globals().pop('videoList')\n","print(\"Variables removed!!!\")"],"metadata":{"id":"qO4kEtrkWYpO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Slide"],"metadata":{"id":"IgR6k8UzljEV"}},{"cell_type":"code","source":["slideList = cleanSlideLogs.select(\"Event\") \\\n","                      .distinct() \\\n","                      .coalesce(1) \\\n","                      .cache()\n","print(\"Total de slides: \", slideList.count())\n","slideList.show(n=15, truncate=False)\n","slideList.write.csv(path=dataPath+\"/Slides\", mode=\"overwrite\", header=True)\n","print(\"CSV written!!!\")\n","slideList.unpersist()\n","globals().pop('slideList')\n","print(\"Variables removed!!!\")"],"metadata":{"id":"tt_oaDKqliZz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Join Sessions and Students"],"metadata":{"id":"8dzRDwCoRS6R"}},{"cell_type":"code","source":["# Obtain sessions list\n","sessions = spark.read.csv(dataPath+\"/Sessions.csv\", header='true', inferSchema='true', sep='\\t') \\\n","                        .withColumn(\"SessionTimestamp\", try_to_timestamp(\"SessionTimestamp\", lit('d/MM/yy, HH:mm'))) \\\n","                        .cache()\n","print(\"Sessions loaded: \"+str(sessions.count()))\n","sessions.show(n=3, truncate=False)"],"metadata":{"id":"JkbKl8mISDmm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Video"],"metadata":{"id":"ga0Fwr559hwV"}},{"cell_type":"code","source":["# Obtain videos per session list\n","videos = spark.read.csv(dataPath+\"/VideosPerSession.csv\", header='true', inferSchema='true', sep=';') \\\n","                        .cache()\n","\n","print(\"Videos per session loaded: \"+str(videos.count()))\n","videos.show(n=3, truncate=False)"],"metadata":{"id":"pmBOzMURRbNS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Enrich the videos with the session information\n","sessionCounter=videos \\\n","                    .withColumnRenamed(\"Session\", \"SessionID\") \\\n","                    .groupBy(\"SessionID\") \\\n","                    .count() \\\n","                    .withColumnRenamed(\"count\", \"TotalVideosInSession\")\n","videosEnriched = videos \\\n","              .join(sessionCounter, videos.Session==sessionCounter.SessionID) \\\n","              .join(sessions, videos.Session==sessions.SessionID) \\\n","              .drop(*['SessionID', 'SessionName']) \\\n","              .withColumnRenamed(\"Event\", \"EventID\") \\\n","              .cache()\n","\n","print(\"Videos enriched: \"+str(videosEnriched.count()))\n","videosEnriched.show(3)"],"metadata":{"id":"GcxEJsLiSkQH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import concat_ws\n","\n","# Obtain students list except NP\n","students = spark.read.csv(dataPath+\"/StudentsNotNP.csv\", header='true', inferSchema='true', sep=';') \\\n","                        .withColumn(\"UsernameID\", concat_ws(\" \",\"First name\", \"Surname\")) \\\n","                        .drop(*[\"ID number\",\"First name\", \"Surname\",\"Institution\",\"Department\",\"Email address\",\"Last downloaded from this course\"]) \\\n","                        .cache()\n","\n","print(\"Students loaded: \"+str(students.count()))\n","students.show(truncate=False)"],"metadata":{"id":"r6X42BlsCOD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","\n","# Join events and videos for all students\n","fullVideos = students.crossJoin(videosEnriched) \\\n","                      .join(cleanVideoLogs, [col(\"EventID\")==col(\"Event\"), col(\"UsernameID\")==col(\"Username\")], 'left_outer') \\\n","                      .select(\"EventID\", \"UsernameID\", \"Session\", \"TotalVideosInSession\", \"SessionTimestamp\", \"Timestamp\", \"VideoRepetitions\") \\\n","                      .fillna(0,\"VideoRepetitions\") \\\n","                      .cache()\n","print(\"Total videos times students: \"+str(fullVideos.count()))\n","fullVideos.show(truncate=False)"],"metadata":{"id":"oNYJEsc_YoJn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summarize logs cardinalities\n","print(\"Total video logs after cleaning: \"+str(cleanVideoLogs.count()))\n","print(\"Total logs after enrichment: \"+str(fullVideos.count())) # This should be larger, because takes all combinations of videos and students"],"metadata":{"id":"9_Cs8DhRc7tH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Slide"],"metadata":{"id":"r3c79lcD9pME"}},{"cell_type":"code","source":["# Obtain sessions list\n","slidesPerSession = spark.read.csv(dataPath+\"/SlidesPerSession.csv\", header='true', inferSchema='true', sep=';') \\\n","                        .cache()\n","print(\"Slides loaded: \"+str(sessions.count()))\n","slidesPerSession.show(n=6, truncate=False)"],"metadata":{"id":"QEWCpU3D92qe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Enrich the slides with the session information\n","fullSlides = slidesPerSession.crossJoin(students.select(\"UsernameID\")) \\\n","              .join(cleanSlideLogs, [col(\"Slide\")==col(\"Event\"), col(\"UsernameID\")==col(\"Username\")], 'left_outer') \\\n","              .join(sessions, col(\"Session\")==col(\"SessionID\")) \\\n","              .drop(\"Event\", \"Slide\", \"Username\", \"SessionID\", \"SessionName\") \\\n","              .withColumnRenamed(\"UsernameId\", \"Username\") \\\n","              .fillna(0,\"SlideRepetitions\") \\\n","              .cache()\n","\n","print(\"Full slides: \"+str(fullSlides.count()))\n","fullSlides.show(n=20, truncate=False)"],"metadata":{"id":"jpwHs6vd_S01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Release memory\n","sessions.unpersist()\n","videos.unpersist()\n","slidesPerSession.unpersist()\n","videosEnriched.unpersist()\n","globals().pop('sessions')\n","globals().pop('videos')\n","globals().pop('slidesPerSession')\n","globals().pop('videosEnriched')\n","print(\"Variables removed!!!\")"],"metadata":{"id":"F7eV9oF0c01n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Feature Engineering"],"metadata":{"id":"JOAx-MZCYF1G"}},{"cell_type":"markdown","source":["### Login"],"metadata":{"id":"qA-cmkTSdlKo"}},{"cell_type":"code","source":["from pyspark.sql.functions import date_add, when, col, sum\n","\n","# Mark with NULL those visualizations not one weeek before the exam\n","loginsAggregatedPerStudent = cleanLoginLogs \\\n","                            .withColumn(\"ExamMinusWeek\", date_add(try_to_timestamp(lit(examDate), lit('yyyy-MM-dd HH:mm:ss')), lit(beforeExamMove))) \\\n","                            .withColumn(\"DuringSessionsLogin\", when(col(\"Timestamp\")<col(\"ExamMinusWeek\"), 1).otherwise(0)) \\\n","                            .withColumn(\"BeforeExamLogin\", when(col(\"Timestamp\")<col(\"ExamMinusWeek\"), 0).otherwise(1)) \\\n","                            .groupBy(\"Username\") \\\n","                            .agg(\n","                                sum(\"DuringSessionsLogin\").alias(\"DuringSessionsLoginCounter\"),\n","                                sum(\"BeforeExamLogin\").alias(\"BeforeExamLoginCounter\")\n","                                ) \\\n","                            .cache()\n","\n","print(\"Total login logs: \"+str(cleanLoginLogs.count()))\n","print(\"Logins per student: \"+str(loginsAggregatedPerStudent.count()))\n","loginsAggregatedPerStudent.show(61)"],"metadata":{"id":"L8sL_q-UeYwN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Video"],"metadata":{"id":"vweZg6xOGgCh"}},{"cell_type":"markdown","source":["#### Before Session"],"metadata":{"id":"FlAMvncFYNkK"}},{"cell_type":"code","source":["from pyspark.sql.functions import when\n","\n","# Mark with NULL those visualizations not before the session\n","beforeSessionVideoLogs = fullVideos \\\n","                            .withColumn(\"Timestamp\", when(col(\"Timestamp\")>=col(\"SessionTimestamp\"), None).otherwise(col(\"Timestamp\"))) \\\n","                            .cache()\n","\n","print(\"Total video logs: \"+str(fullVideos.count()))\n","print(\"Video logs before session: \"+str(beforeSessionVideoLogs.count()))\n","beforeSessionVideoLogs.show(truncate=False)"],"metadata":{"id":"PKcmBtg1YUDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import functions as sf\n","\n","# Compute the percentage of videos visualized before the session\n","beforeSessionVideosAggregatedPerSession = beforeSessionVideoLogs \\\n","                                          .groupBy(\"UsernameID\", \"Session\") \\\n","                                          .agg( sf.count_distinct(\"Timestamp\").alias(\"VideosInSessionCounter\"), \\\n","                                              sf.any_value(\"TotalVideosInSession\").alias(\"TotalVideosInSession\"), \\\n","                                              sf.sum(\"VideoRepetitions\").alias(\"VideoRepetitionsCounter\")\n","                                              ) \\\n","                                          .withColumn(\"BeforeSessionVideoPercent\", sf.col(\"VideosInSessionCounter\")/sf.col(\"TotalVideosInSession\")) \\\n","                                          .withColumn(\"AvgVideoRepetitions\", sf.col(\"VideoRepetitionsCounter\")/sf.col(\"TotalVideosInSession\")) \\\n","                                          .drop(\"VideoRepetitionsCounter\") \\\n","                                          .cache()\n","print(\"Video logs aggregated per session: \"+str(beforeSessionVideosAggregatedPerSession.count())) # This should be students*sessions\n","beforeSessionVideosAggregatedPerSession.show(truncate=False)"],"metadata":{"id":"xsNXBSfpeFZR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### One Week Before Exam"],"metadata":{"id":"SymLPp4GZ5a1"}},{"cell_type":"code","source":["from pyspark.sql.functions import date_add\n","\n","# Mark with NULL those visualizations not one weeek before the exam\n","beforeExamMinusWeekVideoLogs = fullVideos \\\n","                            .withColumn(\"ExamMinusWeek\", date_add(try_to_timestamp(lit(examDate), lit('yyyy-MM-dd HH:mm:ss')), lit(beforeExamMove))) \\\n","                            .withColumn(\"Timestamp\", when(col(\"Timestamp\")>=col(\"ExamMinusWeek\"), None).otherwise(col(\"Timestamp\"))) \\\n","                            .drop(\"ExamMinusWeek\") \\\n","                            .cache()\n","\n","print(\"Total video logs: \"+str(fullVideos.count()))\n","print(\"Video logs one week before exam: \"+str(beforeExamMinusWeekVideoLogs.count()))\n","beforeExamMinusWeekVideoLogs.show()"],"metadata":{"id":"SyBrk0X5aBlV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import functions as sf\n","\n","# Compute the percentage of videos visualized one week before the exam\n","beforeExamMinusWeekVideosAggregatedPerSession = beforeExamMinusWeekVideoLogs \\\n","                                          .groupBy(\"UsernameID\", \"Session\") \\\n","                                          .agg( sf.count_distinct(\"Timestamp\").alias(\"VideosInSessionCounter\"), \\\n","                                              sf.any_value(\"TotalVideosInSession\").alias(\"TotalVideosInSession\")\\\n","                                              ) \\\n","                                          .withColumn(\"BeforeExamMinusWeekVideoPercent\", sf.col(\"VideosInSessionCounter\")/sf.col(\"TotalVideosInSession\"))\n","print(\"Video logs aggregated per session: \"+str(beforeExamMinusWeekVideosAggregatedPerSession.count())) # This should be students*sessions\n","beforeExamMinusWeekVideosAggregatedPerSession.show()\n"],"metadata":{"id":"mx9ACD4vddJw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Before Exam"],"metadata":{"id":"PPmtasXjkNbO"}},{"cell_type":"code","source":["from pyspark.sql.functions import date_add\n","\n","# Mark with NULL those visualizations not before the session\n","beforeExamVideoLogs = fullVideos \\\n","                            .withColumn(\"Exam\", try_to_timestamp(lit(examDate), lit('yyyy-MM-dd HH:mm:ss'))) \\\n","                            .withColumn(\"Timestamp\", when(col(\"Timestamp\")>=col(\"Exam\"), None).otherwise(col(\"Timestamp\"))) \\\n","                            .drop(\"Exam\") \\\n","                            .cache()\n","\n","print(\"Total video logs: \"+str(fullVideos.count()))\n","print(\"Video logs before exam: \"+str(beforeExamVideoLogs.count()))\n","beforeExamVideoLogs.show()\n"],"metadata":{"id":"Fe0aRdfbkUGr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import functions as sf\n","\n","# Compute the percentage of videos visualized one week before the exam\n","beforeExamVideosAggregatedPerSession = beforeExamVideoLogs \\\n","                                          .groupBy(\"UsernameID\", \"Session\") \\\n","                                          .agg( sf.count_distinct(\"Timestamp\").alias(\"VideosInSessionCounter\"), \\\n","                                              sf.any_value(\"TotalVideosInSession\").alias(\"TotalVideosInSession\"), \\\n","                                              sf.sum(\"VideoRepetitions\").alias(\"VideoRepetitionsCounter\")\n","                                              ) \\\n","                                          .withColumn(\"BeforeExamVideoPercent\", sf.col(\"VideosInSessionCounter\")/sf.col(\"TotalVideosInSession\")) \\\n","                                          .withColumn(\"AvgViewedVideoRepetitions\", sf.col(\"VideoRepetitionsCounter\")/sf.col(\"VideosInSessionCounter\")) \\\n","                                          .drop(\"VideoRepetitionsCounter\") \\\n","                                          .cache()\n","print(\"Video logs aggregated per session: \"+str(beforeExamVideosAggregatedPerSession.count())) # This should be students*sessions\n","beforeExamVideosAggregatedPerSession.show()\n","                                          #.fillna(0,\"AvgViewedVideoRepetitions\") \\"],"metadata":{"id":"NDcSwzyNkqbg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Slide"],"metadata":{"id":"AyCvN2NXGnY8"}},{"cell_type":"code","source":["from pyspark.sql.functions import when\n","\n","# Mark with NULL those visualizations not before the session\n","slideAggregates = fullSlides \\\n","                            .withColumn(\"BeforeSessionSlidePercent\", when(col(\"Timestamp\")<col(\"SessionTimestamp\"), 1).otherwise(0)) \\\n","                            .withColumn(\"ExamMinusWeek\", date_add(try_to_timestamp(lit(examDate), lit('yyyy-MM-dd HH:mm:ss')), lit(beforeExamMove))) \\\n","                            .withColumn(\"BeforeExamMinusWeekSlidePercent\", when(col(\"Timestamp\")<col(\"ExamMinusWeek\"), 1).otherwise(0)) \\\n","                            .withColumn(\"Exam\", try_to_timestamp(lit(examDate), lit('yyyy-MM-dd HH:mm:ss'))) \\\n","                            .withColumn(\"BeforeExamSlidePercent\", when(col(\"Timestamp\")<col(\"Exam\"), 1).otherwise(0)) \\\n","                            .drop(\"Exam\",\"ExamMinusWeek\",\"Timestamp\",\"SessionTimestamp\") \\\n","                            .cache()\n","\n","print(\"Total slide logs: \"+str(fullSlides.count()))\n","print(\"Slide logs before session: \"+str(slideAggregates.count()))\n","slideAggregates.show(truncate=False)"],"metadata":{"id":"GbNhh898GmqN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Aggregate per User"],"metadata":{"id":"MzIQjYHtfFW9"}},{"cell_type":"code","source":["from pyspark.sql import functions as sf\n","\n","aggregatedPerUser = beforeSessionVideosAggregatedPerSession \\\n","                                          .join(beforeExamVideosAggregatedPerSession \\\n","                                                .select(*[\"UsernameID\", \"Session\", \"BeforeExamVideoPercent\", \"AvgViewedVideoRepetitions\"]) \\\n","                                                .withColumnRenamed(\"UsernameID\", \"JoinAttr1\") \\\n","                                                .withColumnRenamed(\"Session\", \"JoinAttr2\"), [col(\"UsernameID\")==col(\"JoinAttr1\"), col(\"Session\")==col(\"JoinAttr2\")], 'inner') \\\n","                                          .drop(*[\"JoinAttr1\", \"JoinAttr2\"]) \\\n","                                          .join(beforeExamMinusWeekVideosAggregatedPerSession \\\n","                                                .select(*[\"UsernameID\", \"Session\", \"BeforeExamMinusWeekVideoPercent\"]) \\\n","                                                .withColumnRenamed(\"UsernameID\", \"JoinAttr1\") \\\n","                                                .withColumnRenamed(\"Session\", \"JoinAttr2\"), [col(\"UsernameID\")==col(\"JoinAttr1\"), col(\"Session\")==col(\"JoinAttr2\")], 'inner') \\\n","                                          .drop(*[\"JoinAttr1\", \"JoinAttr2\"]) \\\n","                                          .join(slideAggregates \\\n","                                                .withColumnRenamed(\"Username\", \"JoinAttr1\") \\\n","                                                .withColumnRenamed(\"Session\", \"JoinAttr2\"), [col(\"UsernameID\")==col(\"JoinAttr1\"), col(\"Session\")==col(\"JoinAttr2\")], 'inner') \\\n","                                          .drop(*[\"JoinAttr1\", \"JoinAttr2\"]) \\\n","                                          .groupBy(\"UsernameID\") \\\n","                                          .agg( \\\n","                                              sf.avg(\"BeforeSessionVideoPercent\").alias(\"BeforeSessionVideoPercent\"), \\\n","                                              sf.avg(\"BeforeExamVideoPercent\").alias(\"BeforeExamVideoPercent\"), \\\n","                                              sf.avg(\"BeforeExamMinusWeekVideoPercent\").alias(\"BeforeExamMinusWeekVideoPercent\"), \\\n","                                              sf.avg(\"AvgVideoRepetitions\").alias(\"AvgVideoRepetitions\"), \\\n","                                              sf.avg(\"AvgViewedVideoRepetitions\").alias(\"AvgViewedVideoRepetitions\"), \\\n","                                              sf.avg(\"BeforeSessionSlidePercent\").alias(\"BeforeSessionSlidePercent\"), \\\n","                                              sf.avg(\"BeforeExamSlidePercent\").alias(\"BeforeExamSlidePercent\"), \\\n","                                              sf.avg(\"BeforeExamMinusWeekSlidePercent\").alias(\"BeforeExamMinusWeekSlidePercent\"), \\\n","                                              sf.avg(\"SlideRepetitions\").alias(\"AvgSlideRepetitions\")\n","                                              ) \\\n","                                          .join(loginsAggregatedPerStudent.withColumnRenamed(\"Username\", \"JoinAttr\"), [col(\"UsernameID\")==col(\"JoinAttr\")], 'left') \\\n","                                          .drop(\"JoinAttr\") \\\n","                                          .join(students.withColumnRenamed(\"UsernameID\", \"JoinAttr\"), [col(\"UsernameID\")==col(\"JoinAttr\")], 'inner') \\\n","                                          .drop(\"JoinAttr\") \\\n","                                          .cache()\n","\n","\n","\n","print(\"All together per user: \"+str(aggregatedPerUser.count()))\n","aggregatedPerUser.sort(\"BeforeSessionVideoPercent\").show(n=20, truncate=False)"],"metadata":{"id":"jFXIFJMa_JcM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Release memory\n","beforeSessionVideoLogs.unpersist()\n","beforeExamMinusWeekVideoLogs.unpersist()\n","beforeExamVideosAggregatedPerSession.unpersist()\n","slideAggregates.unpersist()\n","students.unpersist()\n","loginsAggregatedPerStudent.unpersist()\n","globals().pop('beforeSessionVideoLogs')\n","globals().pop('beforeExamMinusWeekVideoLogs')\n","globals().pop('beforeExamVideosAggregatedPerSession')\n","globals().pop('slideAggregates')\n","globals().pop('students')\n","globals().pop('loginsAggregatedPerStudent')\n","print(\"Variables removed!!!\")"],"metadata":{"id":"RAGGdfnNeAie"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Aggregate per Week"],"metadata":{"id":"GMUhdB0OolM1"}},{"cell_type":"code","source":["from pyspark.sql.functions import next_day, col\n","\n","LoginWeeklySerie = cleanLoginLogs \\\n","                      .withColumn(\"Week\", next_day(col(\"Timestamp\"), sessionDay)) \\\n","                      .groupBy(\"Week\") \\\n","                      .count() \\\n","                      .withColumnRenamed(\"count\", \"LoginCount\")\n","LoginWeeklySerie.show()"],"metadata":{"id":"3-YqSdUcoqap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import next_day, col, sum\n","\n","VideoWeeklySerie = cleanVideoLogs \\\n","                      .withColumn(\"Week\", next_day(col(\"Timestamp\"), sessionDay)) \\\n","                      .groupBy(\"Week\") \\\n","                      .agg(sum(\"VideoRepetitions\").alias(\"VideoCount\"))\n","VideoWeeklySerie.show()"],"metadata":{"id":"Tgqs-gYRqO9M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql.functions import next_day, col, sum\n","\n","SlideWeeklySerie = cleanSlideLogs \\\n","                      .withColumn(\"Week\", next_day(col(\"Timestamp\"), sessionDay)) \\\n","                      .groupBy(\"Week\") \\\n","                      .agg(sum(\"SlideRepetitions\").alias(\"SlideCount\"))\n","SlideWeeklySerie.show()"],"metadata":{"id":"-ewrIvTWrkEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aggregatedPerWeek = LoginWeeklySerie \\\n","                      .join(VideoWeeklySerie \\\n","                            .withColumnRenamed(\"Week\", \"JoinAttr\"), [col(\"Week\")==col(\"JoinAttr\")], 'inner') \\\n","                      .drop(\"JoinAttr\") \\\n","                      .join(SlideWeeklySerie \\\n","                            .withColumnRenamed(\"Week\", \"JoinAttr\"), [col(\"Week\")==col(\"JoinAttr\")], 'inner') \\\n","                      .drop(\"JoinAttr\") \\\n","                      .sort(\"Week\")\n","aggregatedPerWeek.show()"],"metadata":{"id":"Hpz9g-_5sOw7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Visualizations"],"metadata":{"id":"5SWDAZw0AGOO"}},{"cell_type":"code","source":["# Set Data Path\n","chartsPath = \"Charts/2425Q1-DBDP1\"\n","# Show Data Folders\n","os.listdir(chartsPath)"],"metadata":{"id":"0uTZFe9fIGj2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move main aggregated dataframe from Spark to Pandas\n","df = aggregatedPerUser.toPandas().set_index(['Gender','UsernameID'])\n","predictors=list(set(df.columns.tolist())-set(['TestMark','ExamMark','ExercisesMark']))\n","df.to_csv(dataPath+'/pd_AnyTime.csv')\n","df.iloc[0:5]"],"metadata":{"collapsed":true,"id":"vfsfhb0nAXAQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Move timeseries from Spark to Pandas\n","ts = aggregatedPerWeek.toPandas().set_index('Week')\n","ts"],"metadata":{"id":"jAy4Ayq9tVde"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prechecks"],"metadata":{"id":"X_RkNR8xXJ1f"}},{"cell_type":"code","source":["# People having a general average of video repetition greater than the average considering only those seen at least once (This should be empty!!!)\n","df[df['AvgVideoRepetitions']>df['AvgViewedVideoRepetitions']]"],"metadata":{"id":"aFv_UHitmWBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# People visualizing videos one week before the exam\n","df[df['BeforeExamMinusWeekVideoPercent']!=df['BeforeExamVideoPercent']]"],"metadata":{"id":"06TwQRAIcaZ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# People reading slides one week before the exam\n","df[df['BeforeExamMinusWeekSlidePercent']!=df['BeforeExamSlidePercent']]"],"metadata":{"id":"SXeTEM41WuYU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Univariate analysis"],"metadata":{"id":"XBf2dMb-Y4DH"}},{"cell_type":"code","source":["df.describe()"],"metadata":{"id":"j1lVd2fYajWC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Bivariate analysis"],"metadata":{"id":"yjA7AeCPe9cU"}},{"cell_type":"code","source":["import seaborn as sns\n","\n","# Check distributions between test questions and exercises in the exam\n","ax = sns.jointplot(y='TestMark',x='ExercisesMark',data=df, height=3)\n","ax.savefig(chartsPath+\"/JointGrid-Text_x_Exercises.pdf\", format=\"pdf\", bbox_inches='tight')"],"metadata":{"id":"jvTRB1VonX5z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Time series"],"metadata":{"id":"8Pl1D3bTmFPT"}},{"cell_type":"code","source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","print(\"Exam date:\", examDate.split()[0])\n","for counter in ts.columns:\n","  fig = plt.figure()\n","  ax=sns.barplot(data=ts, x=counter, y=\"Week\", orient=\"h\")\n","  plt.axhline(lastSessionDate, color=\"g\", linestyle=\":\");\n","  plt.axhline(plannedExamDate, color=\"r\", linestyle=\"--\")\n","  plt.axhline(examDate.split()[0], color=\"r\", linestyle=\"-\")\n","  ax.set(ylabel=\"Semana\")\n","  ax.set(xlabel=\"Número de accesos\")\n","  plt.savefig(chartsPath+\"/TimeSerie-\"+counter+\".pdf\", format=\"pdf\", bbox_inches='tight')\n","  ax.set(xlabel=counter)\n","  #lt.title(counter)\n","  plt.show()"],"metadata":{"id":"jtO3UPT5uSr_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Clustering"],"metadata":{"id":"k_pTnfdZscWd"}},{"cell_type":"markdown","source":["### Agglomerative"],"metadata":{"id":"XQZSzuN8ogPF"}},{"cell_type":"code","source":["# Cluster the marks of students\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import AgglomerativeClustering\n","\n","X = df[['ExercisesMark','TestMark']]\n","clustering = AgglomerativeClustering(n_clusters=4).fit(X)\n","clusters = clustering.fit_predict(X)\n","\n","fig, ax = plt.subplots(figsize=(5, 5))\n","ax.set_title(\"Agglomerative clustering of marks\")\n","ax.set_xlabel(\"ExercisesMark\")\n","ax.set_ylabel(\"TestMark\")\n","scatter = plt.scatter(X['ExercisesMark'], X['TestMark'], c=clusters)\n","plt.legend(handles=scatter.legend_elements()[0], labels=scatter.legend_elements()[1], title=\"Labels\")\n","plt.show()"],"metadata":{"id":"fsy-361JhMJF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace marks by labels\n","dfLabeled = df[predictors]\n","dfLabeled['LabelAgglomerative'] = clusters.tolist()\n","dfLabeled['LabelAgglomerative'].replace({0: \"GoodTest\", 1:\"Bad\", 2: \"Middle\", 3:\"Good\"},inplace=True)\n","dfLabeled"],"metadata":{"id":"EKANeU__mR77","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### K-Means"],"metadata":{"id":"l_9PXlTVol9e"}},{"cell_type":"code","source":["# Cluster the marks of students\n","import matplotlib.pyplot as plt\n","from sklearn.cluster import KMeans\n","\n","X = df[['ExercisesMark','TestMark']]\n","clustering = KMeans(n_clusters=3).fit(X)\n","clusters = clustering.fit_predict(X)\n","\n","fig, ax = plt.subplots(figsize=(5, 5))\n","ax.set_title(\"K-Means clustering of marks\")\n","ax.set_xlabel(\"ExercisesMark\")\n","ax.set_ylabel(\"TestMark\")\n","scatter = plt.scatter(X['ExercisesMark'], X['TestMark'], c=clusters)\n","plt.legend(handles=scatter.legend_elements()[0], labels=scatter.legend_elements()[1], title=\"Labels\")\n","plt.show()"],"metadata":{"id":"BZgsR4RTliTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dfLabeled['LabelKMeans'] = clusters.tolist()\n","dfLabeled['LabelKMeans'].replace({0: \"Good\", 1:\"Bad\", 2: \"GoodTest\"}, inplace=True)\n","dfLabeled"],"metadata":{"id":"F-uiqWEomKfY"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"152jRl_9uzRKHAFKxA3iCHVkqm-wbvHyr","timestamp":1730047716796},{"file_id":"1y9_kvhRO5UK6YAwVH4Ck5gjRnVEF3ihb","timestamp":1730046854130}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
